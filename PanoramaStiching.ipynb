{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation stitches multiple images into a single panorama picture. We will use image features and local descriptors to find corresponding points in image pairs and compute perspective transformations between the 3D image planes. In a last step, all images are warped to the same reference image\n",
    "plane and displayed on the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "#from utils import extract_features, ransac, show_images, save_images, create_stitched_image\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import fargv  #  pip install fargv or comment line 49 {Laziest command-line argument parser}\n",
    "import os\n",
    "import numpy as np \n",
    "from scipy.spatial.distance import cdist\n",
    "from typing import Tuple, Dict, List\n",
    "import copy\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"images_glob\": \"resources/resources/*jpg\",\n",
    "    \"features_per_image\": 500,\n",
    "    \"max_iterations\": 1000,\n",
    "    \"ransac_threshold\": 2.0,\n",
    "    \"significant_descriptor_threshold\": .7,\n",
    "    \"ransac_points\": 4,\n",
    "    \"reference_name\": \"4.jpg\",\n",
    "    \"rendering_order\": [\"4.jpg\", \"3.jpg\", \"5.jpg\", \"2.jpg\", \"6.jpg\", \"1.jpg\", \"7.jpg\", \"0.jpg\", \"8.jpg\"],\n",
    "    \"scale\": 0.8,\n",
    "    \"output_file_name\": \"output.png\"\n",
    "}\n",
    "\n",
    "\n",
    "t_points = np.array\n",
    "t_descriptors = np.array\n",
    "t_homography = np.array\n",
    "t_img = np.array\n",
    "t_images = Dict[str, t_img]\n",
    "t_homographies = Dict[Tuple[str, str], t_homography]  # The keys are the keys of src and destination images\n",
    "t_image_list = List[np.array]\n",
    "t_str_list = List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Images from Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(os.getcwd()+\"resources/resources\", \"results\")\n",
    "load_img = lambda x: cv2.resize(cv2.imread(x), None, fx=parameters[\"scale\"], fy=parameters[\"scale\"])\n",
    "filenames = glob.glob(parameters[\"images_glob\"])\n",
    "images = {os.path.basename(path): load_img(path) for path in filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Feature descriptors from Images\n",
    "\n",
    "To compute a homographic transformation between a pair of images, it is required to have at least four correct point-to-point correspondences. These correspondences can be obtained by matching the feature descriptors of the computed key-points. In this implementation, we use ORB key-points and descriptors. The ORB feature descriptors are uchar\n",
    "vectors with 32 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img: t_img, num_features: int = 500) -> Tuple[t_points, t_descriptors]:\n",
    "    \"\"\"Extracts key-points and their descriptors.\n",
    "    The OpenCV implementation of ORB is used as a backend.\n",
    "    It is based on the FAST key-point detector and a modified version of the visual descriptor BRIEF (Binary Robust Independent Elementary Features).\n",
    "    Its aim is to provide a fast and efficient alternative to SIFT.\n",
    "\n",
    "    Args:\n",
    "        img: a numpy array of [H x Wx 3] size with byte values.\n",
    "        num_features: an integer signifying how many points we desire.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing a numpy array of [N x 2] and numpy array of [N x 32]\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale (ORB works on single-channel images)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize ORB detector with the desired number of features\n",
    "    orb = cv2.ORB_create(nfeatures=num_features)\n",
    "\n",
    "    # Detect key-points and compute descriptors\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray_img, None)\n",
    "\n",
    "    # Convert keypoints to numpy array of coordinates (x, y)\n",
    "    points = np.array([kp.pt for kp in keypoints], dtype=np.float32)\n",
    "\n",
    "    return points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {path: extract_features(images[path], parameters[\"features_per_image\"]) for path in images.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = {path: features[0] for path, features in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = {path: features[1] for path, features in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(descriptors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function finds the significant matches between descriptors in the source and destination image and returns their locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_align_descriptors(f1: Tuple[t_points, t_descriptors], f2: Tuple[t_points, t_descriptors],\n",
    "                                 similarity_threshold=.7, similarity_metric='hamming') -> Tuple[t_points, t_points]:\n",
    "    \"\"\"Aligns pairs of keypoints from two images.\n",
    "    Aligns keypoints from two images based on descriptor similarity.\n",
    "    If K points have been detected in image1 and J points have been detected in image2, the result will be to sets of N\n",
    "    points representing points with similar descriptors; where N <= J and K <=points.\n",
    "\n",
    "    Args:\n",
    "        f1: A tuple of two numpy arrays with the first array having dimensions [N x 2] and the second one [N x M]. M\n",
    "            representing the dimensionality of the point features. In the case of ORB features, M is 32.\n",
    "        f2: A tuple of two numpy arrays with the first array having dimensions [J x 2] and the second one [J x M]. M\n",
    "            representing the dimensionality of the point features. In the case of ORB features, M is 32.\n",
    "        similarity_threshold: The ratio the distance of most similar descriptor in image2 to the distance of the second\n",
    "            most similar ratio.\n",
    "        similarity_metric: A string with the name of the metric by witch distances are calculated. It must be compatible\n",
    "            with the ones that are defined for scipy.spatial.distance.cdist.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of numpy arrays both sized [N x 2] representing the similar point locations.\n",
    "\n",
    "    \"\"\"\n",
    "    assert f1[0].shape[1] == f2[0].shape[1] == 2  # descriptor size\n",
    "    assert f1[1].shape[1] == f2[1].shape[1] == 32  # points size\n",
    "\n",
    "    # step 1: compute distance matrix (1 to 8 lines)\n",
    "    dist_matrix = cdist(f1[1], f2[1], metric=similarity_metric)\n",
    "\n",
    "    # step 2: computing the indexes of src dst so that src[src_idx,:] and dst[dst,:] refer to matching points.\n",
    "    sorted_indices = np.argsort(dist_matrix, axis=1)  # Sort distances for each descriptor in f1\n",
    "    best_indices = sorted_indices[:, 0]  # Best match indices (smallest distance)\n",
    "    second_best_indices = sorted_indices[:, 1]  # Second best match indices (second smallest distance)\n",
    "\n",
    "    # step 3: find a boolean index of the matched pairs that is true only if a match was significant.\n",
    "    # A match is considered significant if the ratio of its distance to the second best is lower than a given\n",
    "    # threshold.\n",
    "    # Hint: use the previously computed distance matrix to find the second best match.\n",
    "    best_distances = dist_matrix[np.arange(dist_matrix.shape[0]), best_indices]  # Best match distances\n",
    "    second_best_distances = dist_matrix[np.arange(dist_matrix.shape[0]), second_best_indices]  # Second best match distances\n",
    "    ratio_test = best_distances / second_best_distances  # Ratio of best to second-best match\n",
    "\n",
    "    valid_matches = ratio_test < similarity_threshold\n",
    "\n",
    "    # step 4: removing non-significant matches and return the aligned points (their location only!)\n",
    "    matched_src_points = f1[0][valid_matches]  # Filtered keypoints from source image\n",
    "    matched_dst_points = f2[0][best_indices[valid_matches]]  # Corresponding keypoints from destination image\n",
    "\n",
    "    return matched_src_points, matched_dst_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A homography is a perspective transformation that maps planes to planes in a three dimensional space. We will compute the homography that transforms the image plane of one camera to the image plane of another camera. These transformations allow us to project all images to the same image\n",
    "plane and to create the final panorama image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_homography(f1: np.array, f2: np.array) -> np.array:\n",
    "    \"\"\"Computes the homography matrix given matching points.\n",
    "\n",
    "    In order to define a homography a minimum of 4 points are needed but the homography can also be overdefined with 5\n",
    "    or more points.\n",
    "\n",
    "    Args:\n",
    "        f1: A numpy array of size [N x 2] containing x and y coordinates of the source points.\n",
    "        f2: A numpy array of size [N x 2] containing x and y coordinates of the destination points.\n",
    "\n",
    "    Returns:\n",
    "        A [3 x 3] numpy array containing normalised homography matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_points = f1.shape[0]\n",
    "    assert num_points >= 4, \"At least 4 points are required to compute the homography.\"\n",
    "\n",
    "    # Homogeneous coordinates\n",
    "    homography_matrix = np.zeros((3, 3))\n",
    "    assert f1.shape[0] == f1.shape[0] >= 4\n",
    "\n",
    "    # TODO 3\n",
    "    # - Construct the (>=8) x 9 matrix A.\n",
    "    # - Use the formula from the exercise sheet.\n",
    "    # - Note that every match contributes to exactly two rows of the matrix.\n",
    "    # - Extract the homogeneous solution of Ah=0 as the rightmost column vector of V.\n",
    "    # - Store the result in H.\n",
    "    # - Normalize H\n",
    "    # Hint: No loops are needed but up to to 2 nested loops might make the solution easier.\n",
    "\n",
    "    A = []\n",
    "    for i in range(num_points):\n",
    "        x, y = f1[i, 0], f1[i, 1]\n",
    "        x_prime, y_prime = f2[i, 0], f2[i, 1]\n",
    "\n",
    "        # First row for this point\n",
    "        A.append([-x, -y, -1, 0, 0, 0, x_prime * x, x_prime * y, x_prime])\n",
    "        # Second row for this point\n",
    "        A.append([0, 0, 0, -x, -y, -1, y_prime * x, y_prime * y, y_prime])\n",
    "\n",
    "    A = np.array(A) # Convert A to a numpy array\n",
    "\n",
    "    U, S, Vt = np.linalg.svd(A) # Step 2: Compute SVD of A\n",
    "\n",
    "    H = Vt[-1].reshape(3, 3) # Step 3: Extract the last column of V (which is Vt[-1]) and reshape to 3x3 matrix\n",
    "    \n",
    "    H = H / H[2, 2] # Step 4: Normalize the homography matrix so that H[2, 2] = 1\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the number of inlier matches given a set of matching point hypotheses (two numpy arrays), the homography matrix\n",
    "and the threshold in pixels. Note that, when applying a perspective transformation, a 2D image point must be temporarily lifted into homogeneous space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_inlier_count(src_points: np.array, dst_points: np.array, homography: np.array,\n",
    "                      distance_threshold: float) -> int:\n",
    "    \"\"\"Computes the number of inliers for a homography given aligned points.\n",
    "    ## - Project the image points from image 1 to image 2\n",
    "    ## - A point is an inlier if the distance between the projected point and\n",
    "    ##      the point in image 2 is smaller than threshold.\n",
    "    Args:\n",
    "        src_points: a numpy array of [N x 2] containing source points.\n",
    "        dst_points: a numpy array of [N x 2] containing source points.\n",
    "        homography: a [3 x 3] numpy array.\n",
    "        distance_threshold: a float representing the norm of the difference between to points so that they will be\n",
    "            considered the same (near enough).\n",
    "\n",
    "    Returns:\n",
    "        An integer counting how many transformed source points matched destination.\n",
    "    \"\"\"\n",
    "    assert src_points.shape[1] == dst_points.shape[1] == 2\n",
    "    assert src_points.shape[0] == dst_points.shape[0]\n",
    "\n",
    "    # step 1: create normalized coordinates for points (maybe [x, y] --> [x, y, 1]) (4 lines)\n",
    "    src_homogeneous = np.hstack([src_points, np.ones((src_points.shape[0], 1))])\n",
    "\n",
    "    # step 2: project the image points from image 1 to image 2 using the homography (1 line)\n",
    "    # Hint: You can use np.dot here\n",
    "    projected_points = np.dot(src_homogeneous, homography.T)\n",
    "\n",
    "    # step 3: re-normalize the projected points ([x, y, l] --> [x/l, y/l]) (1 line)\n",
    "    projected_points = projected_points[:, :2] / projected_points[:, 2].reshape(-1, 1)\n",
    "\n",
    "    # step 4: compute and return number of inliers (3 lines)\n",
    "    # Hint: You might use np.linalg.norm\n",
    "    distances = np.linalg.norm(projected_points - dst_points, axis=1)\n",
    "    \n",
    "    inliers = distances < distance_threshold\n",
    "\n",
    "    return np.sum(inliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after aligning and filtering, some of the feature matches are outliers. If one of the four matches used to compute the homography is an outlier, the resulting transformation matrix is incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the RANSAC inliers and the RANSAC\n",
    "algorithm to achieve a robust result as follows:\n",
    "1. Filter and align descriptors\n",
    "2. In an optimization loop:\n",
    "    a) Select a random subset of at least 4 points\n",
    "    b) Compute the homography for these selected random points\n",
    "    c) See if this homography performs better than any prior homography and, if\n",
    "    so, store it.\n",
    "3. Return the best homography\n",
    "Since RANSAC is not a deterministic algorithm, small variations are to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac(src_features: Tuple[t_points, t_descriptors], dst_features: Tuple[t_points, t_descriptors], steps,\n",
    "           distance_threshold, n_points=4, similarity_threshold=.7) -> np.array:\n",
    "    \"\"\"Computes the best homography given noisy point descriptors.\n",
    "\n",
    "    https://en.wikipedia.org/wiki/Random_sample_consensus\n",
    "    \n",
    "    Args:\n",
    "        src_features: A tuple with points and their descriptors detected in the source image.\n",
    "        dst_features: A tuple with points and their descriptors detected in the destination image.\n",
    "        steps: An integer defining how many iterations to define.\n",
    "        distance_threshold: A float defining how far should to points be to be considered the same.\n",
    "        n_points: The number of point pairs used to compute the homography, it must be grater than 3.\n",
    "        similarity_threshold: The ratio of the most similar descriptor to the second most similar in order to consider\n",
    "            that descriptors from the two images match.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the homography.\n",
    "    \"\"\"\n",
    "\n",
    "    # step 1: filter and align descriptors (1 line)\n",
    "    src_points, dst_points = filter_and_align_descriptors(src_features, dst_features, similarity_threshold)\n",
    "\n",
    "    # step 2: initialize the optimization loop\n",
    "    best_count = 0\n",
    "    best_homography = np.eye(3)\n",
    "\n",
    "    # step 3: optimization loop\n",
    "    for n in range(steps):\n",
    "\n",
    "        # step a: select random subset of points (at least 4 points) (2 lines)\n",
    "        idx = np.random.choice(len(src_points), n_points, replace=False)\n",
    "        src_sample = src_points[idx]\n",
    "        dst_sample = dst_points[idx]\n",
    "\n",
    "        # step b: compute homography for the random points (1 line)\n",
    "        homography, _ = cv2.findHomography(src_sample, dst_sample, method=0)\n",
    "\n",
    "        # step c: compare the current homography to the current best homography and update the best homography using\n",
    "        # inlier count (4 lines)\n",
    "        if homography is not None:\n",
    "            # Step c: Count the number of inliers for this homography\n",
    "            inlier_count = _get_inlier_count(src_points, dst_points, homography, distance_threshold)\n",
    "\n",
    "            # If current homography has more inliers than the best so far, update the best\n",
    "            if inlier_count > best_count:\n",
    "                best_count = inlier_count\n",
    "                best_homography = homography\n",
    "\n",
    "    print(f\"After {steps:4} steps: {best_count} RANSAC points match!\")\n",
    "\n",
    "    # step 4: return the best homography\n",
    "    return best_homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_homographies(homographies: t_homographies, reference_name: str) -> t_homographies:\n",
    "    \"\"\"Computes homographies from every image to the reference image given a homographies between all pairs of\n",
    "    consecutive images.\n",
    "\n",
    "    This method could be loosely described as applying Dijkstra's algorithm applied to exploit the commutative\n",
    "    relationship of matrix multiplication and compute homography matrices between all images and any image.\n",
    "\n",
    "    Args:\n",
    "        homographies: A dictionary where the keys are tuples with the names of each image pair and the values are\n",
    "            [3 x 3] arrays containing the homographies between those images.\n",
    "        reference_name: The of the image which will be the destination for all homographies.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of the same form as the input mappning all images to the reference.\n",
    "    \"\"\"\n",
    "    initial = {k: v for k, v in homographies.items()}  # deep copy\n",
    "    for k, h in list(initial.items()):\n",
    "        initial[(k[1], k[0])] = np.linalg.inv(h)\n",
    "    initial[(reference_name, reference_name)] = np.eye(3)  # Added the identity homography for the reference\n",
    "    desired = set([(k[0], reference_name) for k in homographies.keys()])\n",
    "    solved = {k: v for k, v in initial.items() if k[1] == reference_name}\n",
    "    while not (set(solved.keys()) >= desired):\n",
    "\n",
    "        new_steps = set([(i, s) for i, s in product(initial.keys(), solved.keys()) if\n",
    "                     s[1] != i[0] and s[0] == i[1] and s[0] != s[1] and (i[0], s[1]) not in solved.keys()])\n",
    "        # s[1] != i[0] no pair who's product leads to identity\n",
    "        # s[0] == i[1] only connected pairs\n",
    "        # s[0]!=s[1] no identity in the solution\n",
    "        # set removes duplicates\n",
    "\n",
    "        assert len(new_steps) > 0  # not all desired can be linked to reference\n",
    "        for initial_k, solved_k in new_steps:\n",
    "            new_key = initial_k[0], solved_k[1]\n",
    "            solved[solved_k]\n",
    "            initial[initial_k]\n",
    "            solved[new_key] = np.matmul(solved[solved_k], initial[initial_k])\n",
    "    return solved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_panorama_borders(images: t_images, homographies: t_homographies) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Computes the bounding box of the panorama defined the images and the homographies mapping them to the reference.\n",
    "\n",
    "    This bounding box can have non integer and even negative coordinates.\n",
    "\n",
    "    Args:\n",
    "        images: A dictionary mapping image names to numpy arrays containing images.\n",
    "        homographies:  A dictionary mapping Tuples with pairs image names to numpy arrays representing homographies\n",
    "            mapping from the first image to the second.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the bounding box [left, top, right, bottom] of the whole panorama if stiched.\n",
    "\n",
    "    \"\"\"\n",
    "    homographies = {k[0]: v for k, v in homographies.items()}  # assining homographies to their source image\n",
    "    assert homographies.keys() == images.keys()  # map homographies to source image only\n",
    "    all_corners = []\n",
    "    for name in sorted(images.keys()):\n",
    "        img, homography = images[name], homographies[name]\n",
    "        width, height = img.shape[0], img.shape[1]\n",
    "        corners = ((0, 0), (0, width), (height, width), (height, 0))\n",
    "        corners = np.array(corners, dtype='float32')\n",
    "        all_corners.append(cv2.perspectiveTransform(corners[None, :, :], homography)[0, :, :])\n",
    "    all_corners = np.concatenate(all_corners, axis=0)\n",
    "    left, right = np.floor(all_corners[:, 0].min()), np.ceil(all_corners[:, 0].max())\n",
    "    top, bottom = np.floor(all_corners[:, 1].min()), np.ceil(all_corners[:, 1].max())\n",
    "    return left, top, right, bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_homographies(homographies: t_homographies, dx: float, dy: float):\n",
    "    \"\"\"Applies a uniform translation to a dictionary with homographies.\n",
    "\n",
    "    Args:\n",
    "        homographies: A dictionary mapping Tuples with pairs image names to numpy arrays representing homographies\n",
    "            mapping from the first image to the second.\n",
    "        dx: a float representing the horizontal displacement of the translation.\n",
    "        dy: a float representing the vertical displacement of the translation.\n",
    "\n",
    "    Returns:\n",
    "        a copy of the homographies dict which maps the same keys to the translated matrices.\n",
    "    \"\"\"\n",
    "    # step 1: create a translation matrix (3 lines)\n",
    "    translation_matrix = np.array([\n",
    "        [1, 0, dx],\n",
    "        [0, 1, dy],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # step 2: apply translation matrix on every homography matrix (2 lines)\n",
    "    translated_homographies = copy.deepcopy(homographies)\n",
    "\n",
    "    for key in translated_homographies:\n",
    "        translated_homographies[key] = np.matmul(translation_matrix, translated_homographies[key])\n",
    "\n",
    "    return translated_homographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_panorama(images: t_images, homographies: t_homographies, output_size: Tuple[int, int],\n",
    "                   rendering_order: List[str] = []) -> t_images:\n",
    "    \"\"\"Stiches images after it reprojects them with a homography.\n",
    "\n",
    "    Args:\n",
    "        images: A dictionary mapping image names to numpy arrays containing images.\n",
    "        homographies: A dictionary mapping Tuples with pairs image names to numpy arrays representing homographies\n",
    "            mapping from the first image to the reference image.\n",
    "        output_size: A tuple with integers representing the witdh and height of the resulting panorama.\n",
    "        rendering_order: A list containing the names of the images representing the order in witch the images will be\n",
    "            overlaid. The list must contain either all images names in some permutation or be empty in which case, the\n",
    "            images will be rendered in the alphanumeric order of their names.\n",
    "    Returns:\n",
    "        A numpy array with the panorama image.\n",
    "    \"\"\"\n",
    "    homographies = {k[0]: v for k, v in homographies.items()}  # assining homographies to their source image\n",
    "    assert homographies.keys() == images.keys()\n",
    "    if rendering_order == []:\n",
    "        rendering_order = sorted(images.keys())\n",
    "    panorama = np.zeros([output_size[1], output_size[0], 3], dtype=np.uint8)\n",
    "    for name in rendering_order:\n",
    "        rgba_img = cv2.cvtColor(images[name], cv2.COLOR_RGB2RGBA)\n",
    "        rgba_img[:, :, 3] = 255\n",
    "        tmp = cv2.warpPerspective(rgba_img, homographies[name], output_size, cv2.INTER_LINEAR_EXACT)\n",
    "        new_pixels = ((tmp[:, :, 3] == 255)[:, :, None] & (panorama == np.zeros([1, 1, 3])))\n",
    "        old_pixels = 1 - new_pixels\n",
    "        panorama[:, :, :] = panorama * old_pixels + tmp[:, :, :3] * new_pixels\n",
    "    return panorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stitched_image(images: t_images, homographies: t_homographies, reference_name: str,\n",
    "                          rendering_order: List[str] = []):\n",
    "    \"\"\"Will create a panorama by stitching the input images after reprojecting them.\n",
    "\n",
    "    Args:\n",
    "        images: A dictionary mapping image names to numpy arrays containing images.\n",
    "        homographies: A dictionary mapping Tuples with pairs image names to numpy arrays representing homographies\n",
    "            that can reproject the first image to be aligned with the reference image.\n",
    "        reference_name: A string with the name of the image to which all other images will be aligned.\n",
    "        rendering_order: A list containing the names of the images representing the order in witch the images will be\n",
    "            overlaid. The list must contain either all images names in some permutation or be empty in which case, the\n",
    "            images will be rendered in the alphanumeric order of their names.\n",
    "    Returns:\n",
    "        A numpy array with the panorama image.\n",
    "    \"\"\"\n",
    "    #  from homographies between consecutive images we compute all homographies from any image to the reference.\n",
    "    homographies = propagate_homographies(homographies, reference_name=reference_name)\n",
    "    #  lets calculate the panorama size\n",
    "    left, top, right, bottom = compute_panorama_borders(images, homographies)\n",
    "    width = int(1 + np.ceil(right) - np.floor(left))\n",
    "    height = int(1 + np.ceil(bottom) - np.floor(top))\n",
    "    #  lets make the homographies translate all images inside the panorama.\n",
    "    homographies = translate_homographies(homographies, -left, -top)\n",
    "    return stitch_panorama(images, homographies, (width, height), rendering_order=rendering_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1000 steps: 162 RANSAC points match!\n",
      "After 1000 steps: 179 RANSAC points match!\n",
      "After 1000 steps: 160 RANSAC points match!\n",
      "After 1000 steps: 135 RANSAC points match!\n",
      "After 1000 steps: 139 RANSAC points match!\n",
      "After 1000 steps: 167 RANSAC points match!\n",
      "After 1000 steps: 140 RANSAC points match!\n",
      "After 1000 steps: 169 RANSAC points match!\n"
     ]
    }
   ],
   "source": [
    "names = list(sorted(images.keys()))\n",
    "homographies = {}\n",
    "for n in range(1, len(names)):\n",
    "        name1 = names[n-1]\n",
    "        name2 = names[n]\n",
    "        homographies[(name1, name2)] = ransac((points[name1], descriptors[name1]),\n",
    "                                                  (points[name2], descriptors[name2]),\n",
    "                                                  steps=parameters[\"max_iterations\"], distance_threshold=parameters[\"ransac_threshold\"],\n",
    "                                                  similarity_threshold=parameters[\"significant_descriptor_threshold\"],\n",
    "                                                  n_points=parameters[\"ransac_points\"])\n",
    "panorama = create_stitched_image(images, homographies, parameters[\"reference_name\"], parameters[\"rendering_order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_directory(dir_path):\n",
    "    try:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {dir_path} - {e.strerror}\")\n",
    "\n",
    "        \n",
    "def show_images(images: t_image_list, names: t_str_list) -> None:\n",
    "    \"\"\"Shows one or more images at once.\n",
    "\n",
    "    Displaying a single image can be done by putting it in a list.\n",
    "\n",
    "    Args:\n",
    "        images: A list of numpy arrays in opencv format [HxW] or [HxWxC]\n",
    "        names: A list of strings that will appear as the window titles for each image\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for image_index in range(0, len(images)):\n",
    "        cv2.imshow(names[image_index], images[image_index])\n",
    "        cv2.waitKey(0)\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "def save_images(images: t_image_list, filenames: t_str_list, **kwargs) -> None:\n",
    "    \"\"\"Saves one or more images at once.\n",
    "\n",
    "    Saving a single image can be done by putting it in a list.\n",
    "\n",
    "    Args:\n",
    "        images: A list of numpy arrays in opencv format [HxW] or [HxWxC]\n",
    "        filenames: A list of strings where each respective file will be created\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for image_index in range(0, len(images)):\n",
    "        file_name = filenames[image_index]\n",
    "        _create_directory(os.path.dirname(file_name))\n",
    "\n",
    "        cv2.imwrite(file_name, images[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([panorama], [\"output\"])\n",
    "save_images([panorama], [os.path.join(output_path, parameters[\"output_file_name\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
